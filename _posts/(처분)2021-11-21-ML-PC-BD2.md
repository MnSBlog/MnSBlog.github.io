---
date: 2021-11-21 21:57:00
layout: post
title: Bayesian Classifiers
subtitle: Understaning the Bayesian Classifiers
description: Pattern Classfication
image: https://drive.google.com/uc?export=view&id=175V7McgIY7HevVwnOiksV9yYGgbRSBbr
category: Machine Learning
tags:
  - Machine Learning
  - Pattern Classification
  - Bayesian
  - Analytically method
  - Deviation
author: 안상현
---



[Pattern Classification의 Bayesian Theory](https://mnsblog.github.io/ML-PC-BD1/)에서 이어집니다.

# <span style="color:#2E64FE">Classifiers, Discriminant Functions and Decision Surfaces</span>

## <span style="color:#FF8080">The Multi-Category Case</span>

`Pattern Classifiers` 를 표현하는 방법에는 여러가지가 있습니다. 유용한 것들 중에서 하나는 `Discriminant function g(x)` 의 집합식으로 표현하는 건데요. 간단히 아래와 같습니다.


$$
g_i(\text{x})\gt g_j(\text{x}),\ \forall\ j\neq i\\
i=1,\dots,c\\
\text{feature vector x}
$$


수식으로 표현한 위 식이 `network machine` 으로도 표현할 수 있습니다.

<img src="https://drive.google.com/uc?export=view&id=175V7McgIY7HevVwnOiksV9yYGgbRSBbr" align="center">

`Bayes classifier` 는 쉽고 자연스럽게 느껴지는 위 방법으로 표현합니다. 일반적으로 `risks`를 포함한 형태로 말이죠.
$$
g_i(x)=-R(\alpha_i|x)
$$
는 조건부 `risk` 와 연결됩니다. 위 그림을 보면 모든 각 부분의 g(x)는 x가 주어질때 action으로 내어주는 것이 결국 `omega` 와 이어지게 됩니다. 그래서 다시 이렇게 표현할 수 있죠.
$$
g_i(\text{x})=P(\omega_i|\text{x})
$$
 로요. 결국 `Discriminant function` 은 `posterior probability` 와 이어집니다.

 확실히 `Discriminant function` 을 선택하는 것은 `unique` 하지 않습니다. `Discriminant function` 에 동일한 상수 혹은 함수값을 곱할 수 있습니다. 일반적으로 f(x)라는 함수가 있고 이를 g(x)를 매핑한다고 하겠습니다. f(g(x))는 단순하게 증가하는 함수로 동작한다 할때, 계산은 역시나 단순해집니다. 특히 오류 최소화하는 분류전략에서는 동일한 분류 성능에서 이해나, 계산측면으로 간단한 방법들이 있습니다. `logarithm` 으로 예시를 들겠습니다.


$$
\text{Natural logarithm}\\
g_i(\text{x})=P(\omega_i|\text{x})=\frac{p(\text{x}|\omega_i)P(\omega_i)}{\underset{j=1}{\overset{c}{\sum}}p(\text{x}|\omega_j)P(\omega_j)}\\
g_i(\text{x})=p(\text{x}|\omega_i)P(\omega_i)\\
g_i(\text{x})=\ln p(\text{x}|\omega_i)+\ln P(\omega_i)
$$

#### Decision Region

어떤 `decision rule` 의 효과로 `Feature space` 를 카테고리의 수인 c 만큼 나눈 영역을 `decision region` 이라 부릅니다. 이를
$$
\mathcal{R_1,\dots, R_c}
$$
로 표현하며 시간이 더 있는 사람들은 아래와 같은 이쁜 색상으로 구분합니다. (섹션 제목은 `Multi categories` 였나 싶을 정도의 예시 그림은 2-category..)

<img src="https://drive.google.com/uc?export=view&id=1SP1fnxTO5a6MvtA22ayj7zMIKBLQbuUa" align="center">



### <span style="color:#FF8080">The Two-Category Case</span>

`Two-category` 의 경우는 사실 `Multi categories` 의 경우에서 특별한 객체로서 볼 수 있습니다. 상속받은 자식 클래스 느낌이죠.

#### Dichotomizer

이 경우에서 `classifier` 에는 특별한 이름이 있습니다. `Dichotomizer` 이죠. 또한 딱 두개의 `discriminant function` 만을 사용합니다.

 
$$
g(\text{x})\equiv g_1(x)-g_2(x)
$$
g(x)를 두 `discrimant function` 의 차이로 정의를 하고,


$$
g(\text{x})=P(\omega_1|\text{x})-P(\omega_2|\text{x})\\
g(\text{x})=\ln\frac{p(\text{x}|\omega_1)}{p(\text{x}|\omega_2)}+\ln\frac{P(\omega_1)}{P(\omega_2)}
$$


g(x)를 `Posterior` 로 표현한 것과, `logarithm likelihood + prior` 로 표현한 등식입니다.

# <span style="color:#2E64FE">The Normal Density</span>

해당 부분은 사전 지식으로서 수식과 도표만 넣어두겠습니다.

<details>
<summary>2021</summary>
<div markdown="1">

#### Expectation


$$
\mathcal{E}[f(x)]\equiv\int_{-\infty}^{\infty}f(x)p(x)dx\\
\text{When set D from a discrete distribution}\\
\mathcal{E}[f(x)]=\sum_{x\in\mathcal{D}}f(x)P(x)
$$




## <span style="color:#FF8080">Univariate Density</span>


$$
p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{1}{2}(\frac{x-\mu}{\sigma})\right]\\
\mu \equiv\mathcal{E}[x]=\int_{-\infty}^{\infty}xp(x)\ dx\\
\sigma^2\equiv\mathcal{E}[(x-\mu)^2]=\int_{-\infty}^{\infty}(x-\mu)^2p(x)\ dx
$$


<img src="https://drive.google.com/uc?export=view&id=1NtTbZ2vTE_6iOxYgQ9KWM--nLRWKo4v8" align="center">



#### Variance

#### Mean

#### Entropy


$$
H(p(x))\equiv-\int p(x)\ln p(x)\ dx
$$




#### Central Limit Theorem

## <span style="color:#FF8080">Multivariate Density</span>


$$
p(\text{x})=\frac{1}{(2\pi)^{d/2}|\sum|^{1/2}}\exp\left[-\frac{1}{2}(\text{x}-\mu)^t\Sigma^{-1}(\text{x}-\mu)\right]
$$




#### Covariance Matrix

#### Inner product


$$
\text{a}^tb=\sum_{i=1}^d a_ib_i
$$

$$
\mu \equiv\mathcal{E}[\text{x}]=\int \text{x}p(\text{x}) \text{ dx}
$$

$$
\Sigma \equiv \mathcal{E}[(\text{x}-\mu)(\text{x}-\mu)^t]=\int(\text{x}-\mu)(\text{x}-\mu)^tp(x) \text{ dx}
$$

$$
\mu_i =\mathcal{E}[x_i]
$$

$$
\sigma_{ij}=\mathcal{E}[(x_i-\mu_i)(x_j-\mu_j)]
$$




#### Covariance

#### Statistical Independence

#### Whitening transform


$$
r^2=(\text{x}-\mu)^t\Sigma^{-1}(\text{x}-\mu)
$$




#### Mahalanobis distance


$$
V=V_d|\Sigma|^{1/2}r^d\\
V_d=
\begin{cases}
\pi^{d/2}/(d/2)! &d\mbox{ even}\\
2^d\pi^{(d-1)/2}\frac{d-1}{2}!/(d)! &d \mbox{ odd.}\end{cases}
$$




<img src="https://drive.google.com/uc?export=view&id=1o4-nfX9omcRPecH2hzdi_dIxkCFLFW22" align="center">

</div>

</details>

# <span style="color:#2E64FE">Discriminant Functions for the Normal Density</span>

`Minimum-error rate classification` 을 `Discriminant function` 으로 쓸 수 있다는 것을 위에서 확인했습니다.


$$
g_i(\text{x})=\ln p(\text{x}|\omega_i)+\ln P(\omega_i)
$$


위 표현식은 `densities` 
$$
p(\text{x}|\omega_i)
$$
가 `multivariate normal` 로서 평가가능하다는 것을 알 수 있습니다. 예를 들어서 
$$
p(\text{x}|\omega_i)\sim N(\mu_i,\Sigma_i)
$$
인 조건에서는 아래처럼 가져다 쓸 수 있죠.


$$
g_i(\text{x})=-\frac{1}{2}(\text{x}-\mu_i)^t\Sigma_i^{-1}(\text{x}-\mu_i)-\frac{d}{2}\ln 2\pi-\frac{1}{2}\ln|\Sigma_i|+\ln P(\omega_i)
$$


 길죠.. 그치만 이번 섹션의 젤 위에 있는 식을 풀어썼을 뿐입니다.

## <span style="color:#FF8080">Case 1</span>


$$
\text{Case 1:}\quad\Sigma_i=\sigma^2I
$$


분포에 들어간 `Sigma` 가 `Variance` 로 구성되면 어떻게 되는가?에 대한 얘기입니다. 

<img src="https://drive.google.com/uc?export=view&id=1r2rM_q4s4S5tD1Z8SoSc_v1BdKLJzuYH" align="center">

`Geometrically` `sigma` 가 `variance` 로 바뀐다면, 위에서 x-`mu` 로 표현되는 것들이 제곱이 되죠. 다시 식을 정의하면,


$$
g_i(\text{x})=-\frac{||\text{x}-\mu_i||^2}{2\sigma^2}+\ln P(\omega_i)
$$


로 표현됩니다. 이는 곧 `Euclidean norm(2-norm)` 이죠.

#### Euclidean norm


$$
||\text{x}-\mu_i||^2=(x-\mu_i)^t(\text{x}-\mu_i)
$$
`Prior` 확률이 같지 않으면, g_i(x)의 식은 `Euclidean norm` 으로서 `normaized` 되는 형상을 보여주고 ln P(`omega_i`)가 `offset` 이 됩니다. 

이제 이 `quaratic form` 을 전개하겠습니다.


$$
g_i(\text{x})=-\frac{1}{2\sigma^2}[\text{x}^t\text{x}-2\mu_i^t\text{x}+\mu_i^t\mu_i]+\ln P(\omega_i)
$$




#### Linear discriminant

#### Threshold

#### Bias

#### Linear machine

#### Minimum distance classifier

#### Template-matching

## <span style="color:#FF8080">Case 2</span>

## <span style="color:#FF8080">Case 3</span>

#### Hyper quadric

# <span style="color:#2E64FE">Error Probabilities and Integrals</span>

 지금까지 

# <span style="color:#2E64FE">Error Bounds for Normal Densities</span>

## <span style="color:#FF8080">Chernoff bound</span>

## <span style="color:#FF8080">Signal Detection Theory and Operating Characteristics</span>

# <span style="color:#2E64FE">Bayes Decision Theory - Discrete Features</span>

## <span style="color:#FF8080">Independent Binary Features</span>
